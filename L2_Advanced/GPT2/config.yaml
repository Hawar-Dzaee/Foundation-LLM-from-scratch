dropout: 0.0

# TOKENIZER
vocab_size: 50257   # tiktoken.get_encoding("gpt2").n_vocab
embedding_dim: 4

# DATASET
stride: 3

# DATALOADER
batch_size: 2
shuffle: False
drop_last: True
num_workers: 0

# POSITIONAL ENCODING
# Durning training, num_tokens == context_window
# Durning inference, num_tokens <= context_window
context_window: 100

# Mulit-head attention
num_heads: 2
Q_K_V_bias: False
kv_bias: False
batch_first: True
device: null

# Transformer block
n_layers: 2
