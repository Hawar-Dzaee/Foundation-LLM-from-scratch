import torch
from torch import nn


# def get_token_embeddings(vocab_size,embedding_dim):
#     return nn.Embedding(vocab_size,embedding_dim)

    
    
    

# class TokenEmbedding(nn.Module):
#     def __init__(self,vocab_size,embedding_dim):
#         super().__init__()
#         self.embedding = get_token_embeddings(vocab_size,embedding_dim)
        
#     def forward(self,x):
#         return self.embedding(x)
    
    
    
    
    
    