{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic \n",
    "\n",
    "This notebook reproduces pytorch's built-in module `nn.MultiheadAttention`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration\n",
    "  - Identical hyperparameters and inputs used for both parts\n",
    "\n",
    "\n",
    "Implementation Methods\n",
    "\n",
    "  - Part One: PyTorch's Built-in Implementation\n",
    "  - Part Two: Custom Implementation from Scratch\n",
    "\n",
    "\n",
    "Validation\n",
    "\n",
    "  - Results comparison between Part One and Part Two\n",
    "  - Verification of output equality\n",
    "\n",
    "Afterwords \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identical Hyperparameters for PART ONE and PART TWO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identical inputs (with `torch.manual_seed`) for PART ONE and PART TWO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# why different inputs to produce Q,K,V ?? \n",
    "# stay tunned. \n",
    "\n",
    "\n",
    "x = torch.tensor(\n",
    "    [[[ 0.2096,  1.4551, -0.3562, -1.3084],\n",
    "    [ 1.1463,  0.5509, -1.5349, -0.1624],\n",
    "    [-0.3144,  1.7046, -0.5748, -0.8154],\n",
    "    [ 1.6361, -0.5812, -0.0645, -0.9905]]]\n",
    "         )\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "\n",
    "batch_size,num_tokens,embed_dim = x.shape\n",
    "\n",
    "\n",
    "input_to_produce_Q = x \n",
    "input_to_produce_K = x \n",
    "input_to_produce_V = x \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q = nn.Parameter(torch.tensor([\n",
    "    [-0.5, 0.2, 0.7, -0.9],\n",
    "    [0.1, -0.3, 0.8, 0.4],\n",
    "    [-0.7, 0.6, -0.2, 0.9],\n",
    "    [0.3, -0.8, 0.5, -0.1]\n",
    "]))\n",
    "\n",
    "\n",
    "W_k = nn.Parameter(torch.tensor([\n",
    "    [0.3, -0.5, 0.2, 0.7],\n",
    "    [-0.4, 0.1, -0.6, -0.2],\n",
    "    [0.8, -0.3, 0.5, -0.7],\n",
    "    [-0.1, 0.6, -0.9, 0.4]\n",
    "]))\n",
    "\n",
    "W_v = nn.Parameter(torch.tensor([\n",
    "    [0.2, -0.8, 0.3, 0.5],\n",
    "    [-0.7, 0.4, -0.1, -0.6],\n",
    "    [0.9, -0.2, 0.7, -0.3],\n",
    "    [-0.5, 0.1, -0.4, 0.8]\n",
    "]))\n",
    "\n",
    "out_proj = nn.Parameter(torch.tensor([\n",
    "    [0.5, -0.3, 0.4, 0.2],\n",
    "    [-0.6, 0.4, -0.2, -0.5], \n",
    "    [0.3, -0.7, 0.6, -0.4],\n",
    "    [-0.2, 0.5, -0.4, 0.3]\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART ONE : Pytorch's  Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are W_q,W_k,W_v concatenated into one matrix, this is how pytorch initializes the weight matrix\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.5000,  0.1000, -0.7000,  0.3000],\n",
      "        [ 0.2000, -0.3000,  0.6000, -0.8000],\n",
      "        [ 0.7000,  0.8000, -0.2000,  0.5000],\n",
      "        [-0.9000,  0.4000,  0.9000, -0.1000],\n",
      "        [ 0.3000, -0.4000,  0.8000, -0.1000],\n",
      "        [-0.5000,  0.1000, -0.3000,  0.6000],\n",
      "        [ 0.2000, -0.6000,  0.5000, -0.9000],\n",
      "        [ 0.7000, -0.2000, -0.7000,  0.4000],\n",
      "        [ 0.2000, -0.7000,  0.9000, -0.5000],\n",
      "        [-0.8000,  0.4000, -0.2000,  0.1000],\n",
      "        [ 0.3000, -0.1000,  0.7000, -0.4000],\n",
      "        [ 0.5000, -0.6000, -0.3000,  0.8000]], requires_grad=True)\n",
      "torch.Size([12, 4])\n",
      "--------------------------------------------------------------------------------\n",
      "THe following is out projection weight\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.5000, -0.6000,  0.3000, -0.2000],\n",
      "        [-0.3000,  0.4000, -0.7000,  0.5000],\n",
      "        [ 0.4000, -0.2000,  0.6000, -0.4000],\n",
      "        [ 0.2000, -0.5000, -0.4000,  0.3000]], requires_grad=True)\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim,\n",
    "                                       num_heads=num_heads,\n",
    "                                       dropout=0,\n",
    "                                       bias = False,\n",
    "                                       add_bias_kv=False,\n",
    "                                       batch_first=True,    # important\n",
    "                                       device=None)\n",
    "\n",
    "\n",
    "\n",
    "multihead_attn.in_proj_weight = nn.Parameter(torch.concat([W_q.T,W_k.T,W_v.T]))\n",
    "# To grab initialized `nn.MultiheadAttention`s weights.....\n",
    "print('The following are W_q,W_k,W_v concatenated into one matrix, this is how pytorch initializes the weight matrix\\n')\n",
    "print(multihead_attn.in_proj_weight)\n",
    "print(multihead_attn.in_proj_weight.shape)\n",
    "\n",
    "\n",
    "\n",
    "print('----'*20)\n",
    "\n",
    "multihead_attn.out_proj.weight = nn.Parameter(out_proj.T)\n",
    "print('THe following is out projection weight\\n')\n",
    "print(multihead_attn.out_proj.weight)  # Output projection weights\n",
    "print(multihead_attn.out_proj.weight.shape)  # Output projection shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few words on `nn.MultiheadAttention` arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if `bias` = True and `add_kv_bias` = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q = x \\cdot W^Q + b^Q$\n",
    "\n",
    "$K = x \\cdot W^K + b^K + \\text{bias}_{K_shared}$\n",
    "\n",
    "$V = x \\cdot W^V + b^V + \\text{bias}_{V_shared}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for simplicity we'll set both `bias` and `add_kv_bias` to `False` in `nn.MultiheadAttention`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : pytorch `nn.MultiheadAttention()` does not mask (Q.K^T) by default. we have to pass the argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = torch.triu(torch.ones(num_tokens,num_tokens),diagonal=1).bool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART ONE RESULT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch's nn.MultiheadAttention is designed to be flexible for different use cases, so it requires Q, K, and V as inputs rather than assuming they will be derived from the same source every time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    PART_ONE_RESULT, attn_output_weights_part_one  = multihead_attn(input_to_produce_Q,\n",
    "                                                                    input_to_produce_K,\n",
    "                                                                    input_to_produce_V,\n",
    "                                                                    attn_mask = attn_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll use the weights which `nn.MultiheadAttention` initialized, in our `FromScratchMultiheadAttention`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART TWO : FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FromScratchMultiheadAttention(nn.Module):\n",
    "  def __init__(self,context_window,embed_dim,num_heads,dropout=0,add_bias_kv=False,device=None):\n",
    "    super().__init__()\n",
    "\n",
    "    # we will assume d_in == d_out and they are both embed_dim.\n",
    "\n",
    "    # Handling dimensions\n",
    "    assert embed_dim % num_heads == 0, 'Embedding must be divisible by Number of heads'\n",
    "    self.embed_dim = embed_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = self.embed_dim//self.num_heads\n",
    "\n",
    "\n",
    "    # Miscellaneous\n",
    "    self.register_buffer('mask',torch.triu(torch.ones(context_window,context_window),diagonal=1))\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "  \n",
    "  def forward(self,input_to_produce_Q,input_to_produce_K,input_to_produce_V):\n",
    "    B_q,num_token_q,_ = input_to_produce_Q.shape\n",
    "    B_k,num_token_k,_ = input_to_produce_K.shape\n",
    "    B_v,num_token_v,_ = input_to_produce_V.shape\n",
    "\n",
    "    Q = input_to_produce_Q @ W_q \n",
    "    K = input_to_produce_K @ W_k \n",
    "    V = input_to_produce_V @ W_v \n",
    "\n",
    "\n",
    "    # splitting (turn it to multi-head)\n",
    "    Q = Q.view(B_q,num_token_q,self.num_heads,self.head_dim).transpose(1,2)\n",
    "    K = K.view(B_k,num_token_k,self.num_heads,self.head_dim).transpose(1,2)\n",
    "    V = V.view(B_v,num_token_v,self.num_heads,self.head_dim).transpose(1,2)\n",
    "\n",
    "    # QK,mask,softmax,dropout\n",
    "    attn_score = Q @ K.transpose(2,3)\n",
    "    attn_score.masked_fill_(self.mask.bool()[:num_token_q,:num_token_k],-torch.inf)\n",
    "    attn_weight = torch.softmax(attn_score/K.shape[-1]**0.5,dim=-1)\n",
    "    attn_weight = self.dropout(attn_weight)\n",
    "\n",
    "    # context_vec\n",
    "    context_vec = attn_weight @ V\n",
    "\n",
    "    # Putting the heads back together \n",
    "    context_vec = context_vec.transpose(1,2).contiguous().view(B_q,num_token_q,self.embed_dim)    # it doesn't matter which (B) you choose\n",
    "\n",
    "    # projection \n",
    "    context_vec = context_vec @ out_proj\n",
    "\n",
    "    return context_vec,attn_weight\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART TWO RESULT  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can think of context_window as a max num_tokens your model can process at one go. since  we are feeding the model 8 tokens (numz-tokens = 8), context_window anything above 8 will do. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = FromScratchMultiheadAttention(context_window=1024,   # see the above note.\n",
    "                                    embed_dim=embed_dim,\n",
    "                                    num_heads=num_heads,\n",
    "                                    dropout=0.)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    PART_TWO_RESULT,attn_output_weights_part_two = mha(input_to_produce_Q,input_to_produce_K,input_to_produce_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output_weights_part_two = attn_output_weights_part_two.mean(dim=1)   # taking average across heads dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing `PART_ONE_RESULT` with `PART_TWO_RESULT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of PART ONE :\n",
      "\n",
      "tensor([[[-0.1353, -0.6532,  0.4699, -0.8950],\n",
      "         [-0.1263, -0.6508,  0.3784, -0.7918],\n",
      "         [-0.6962,  0.3417, -0.4282, -0.4440],\n",
      "         [-0.5026, -0.2205,  0.0192, -0.8127]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "Output of PART TWO :\n",
      "\n",
      "tensor([[[-0.1353, -0.6532,  0.4699, -0.8950],\n",
      "         [-0.1263, -0.6508,  0.3784, -0.7918],\n",
      "         [-0.6962,  0.3417, -0.4282, -0.4440],\n",
      "         [-0.5026, -0.2205,  0.0192, -0.8127]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "PART ONE is equal to PART TWO ? True\n"
     ]
    }
   ],
   "source": [
    "print(f'Output of PART ONE :\\n\\n{PART_ONE_RESULT}')\n",
    "print('---'*30)\n",
    "print(f'Output of PART TWO :\\n\\n{PART_TWO_RESULT}')\n",
    "print('---'*30)\n",
    "print(f'\\nPART ONE is equal to PART TWO ? {torch.allclose(PART_ONE_RESULT, PART_TWO_RESULT)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing `attn_output_weights_part_one` with `attn_output_weights_part_two`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of PART ONE :\n",
      "\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.7915, 0.2085, 0.0000, 0.0000],\n",
      "         [0.3695, 0.3792, 0.2513, 0.0000],\n",
      "         [0.3029, 0.1970, 0.4318, 0.0683]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "Output of PART TWO :\n",
      "\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.7915, 0.2085, 0.0000, 0.0000],\n",
      "         [0.3695, 0.3792, 0.2513, 0.0000],\n",
      "         [0.3029, 0.1970, 0.4318, 0.0683]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "PART ONE is equal to PART TWO ? True\n"
     ]
    }
   ],
   "source": [
    "print(f'Output of PART ONE :\\n\\n{attn_output_weights_part_one}')\n",
    "print('---'*30)\n",
    "print(f'Output of PART TWO :\\n\\n{attn_output_weights_part_two}')\n",
    "print('---'*30)\n",
    "print(f'\\nPART ONE is equal to PART TWO ? {torch.allclose(attn_output_weights_part_one, attn_output_weights_part_two)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afterwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feel comfortable proceeding with `nn.MultiheadAttention`, knowing exactly how the math is calculated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
