{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic \n",
    "\n",
    "This notebook reproduces pytorch's built-in module `nn.MultiheadAttention`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration\n",
    "  - Identical hyperparameters and inputs used for both parts\n",
    "\n",
    "\n",
    "Implementation Methods\n",
    "\n",
    "  - Part One: PyTorch's Built-in Implementation\n",
    "  - Part Two: Custom Implementation from Scratch\n",
    "\n",
    "\n",
    "Validation\n",
    "\n",
    "  - Results comparison between Part One and Part Two\n",
    "  - Verification of output equality\n",
    "\n",
    "Afterwords \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identical Hyperparameters for PART ONE and PART TWO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identical inputs (with `torch.manual_seed`) for PART ONE and PART TWO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# why different inputs to produce Q,K,V ?? \n",
    "# stay tunned. \n",
    "\n",
    "\n",
    "x = torch.tensor(\n",
    "    [[[ 0.2096,  1.4551, -0.3562, -1.3084],\n",
    "    [ 1.1463,  0.5509, -1.5349, -0.1624],\n",
    "    [-0.3144,  1.7046, -0.5748, -0.8154],\n",
    "    [ 1.6361, -0.5812, -0.0645, -0.9905]]]\n",
    "         )\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "\n",
    "batch_size,num_tokens,embed_dim = x.shape\n",
    "# input_to_produce_Q = torch.rand(batch_size,num_tokens,embed_dim)\n",
    "# input_to_produce_K = torch.rand(batch_size,num_tokens,embed_dim)\n",
    "# input_to_produce_V = torch.rand(batch_size,num_tokens,embed_dim)\n",
    "\n",
    "input_to_produce_Q = x \n",
    "input_to_produce_K = x \n",
    "input_to_produce_V = x \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART ONE : Pytorch's  Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are W_q,W_k,W_v concatenated into one matrix, this is how pytorch initializes the weight matrix\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.0888, -0.0024,  0.5353,  0.1906],\n",
      "        [-0.2281, -0.3698, -0.1026, -0.2641],\n",
      "        [-0.1962,  0.0293,  0.3651,  0.3328],\n",
      "        [-0.5986,  0.3796,  0.1711,  0.5809],\n",
      "        [ 0.4042, -0.5580, -0.5822, -0.2954],\n",
      "        [ 0.5377, -0.1020,  0.2621, -0.2846],\n",
      "        [ 0.6009, -0.2591,  0.4592,  0.0073],\n",
      "        [-0.3226,  0.3148, -0.3251,  0.1801],\n",
      "        [-0.1768, -0.0671, -0.5887, -0.2920],\n",
      "        [ 0.3323, -0.1489,  0.6100,  0.4909],\n",
      "        [-0.0287, -0.4087,  0.3729,  0.1901],\n",
      "        [-0.3958,  0.3978,  0.3718,  0.5431]], requires_grad=True)\n",
      "torch.Size([12, 4])\n",
      "--------------------------------------------------------------------------------\n",
      "THe following is out projection weight\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.2576, -0.2207, -0.0969,  0.2347],\n",
      "        [-0.4707,  0.2999, -0.1029,  0.2544],\n",
      "        [ 0.0695, -0.0612,  0.1387,  0.0247],\n",
      "        [ 0.1826, -0.1949, -0.0365, -0.0450]], requires_grad=True)\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim,\n",
    "                                       num_heads=num_heads,\n",
    "                                       dropout=0,\n",
    "                                       bias = False,\n",
    "                                       add_bias_kv=False,\n",
    "                                       batch_first=True,    # important\n",
    "                                       device=None)\n",
    "\n",
    "\n",
    "\n",
    "# To grab initialized `nn.MultiheadAttention`s weights.....\n",
    "print('The following are W_q,W_k,W_v concatenated into one matrix, this is how pytorch initializes the weight matrix\\n')\n",
    "print(multihead_attn.in_proj_weight)\n",
    "print(multihead_attn.in_proj_weight.shape)\n",
    "\n",
    "\n",
    "print('----'*20)\n",
    "\n",
    "print('THe following is out projection weight\\n')\n",
    "print(multihead_attn.out_proj.weight)  # Output projection weights\n",
    "print(multihead_attn.out_proj.weight.shape)  # Output projection shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few words on `nn.MultiheadAttention` arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if `bias` = True and `add_kv_bias` = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q = x \\cdot W^Q + b^Q$\n",
    "\n",
    "$K = x \\cdot W^K + b^K + \\text{bias}_{K_shared}$\n",
    "\n",
    "$V = x \\cdot W^V + b^V + \\text{bias}_{V_shared}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for simplicity we'll set both `bias` and `add_kv_bias` to `False` in `nn.MultiheadAttention`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : pytorch `nn.MultiheadAttention()` does not mask (Q.K^T) by default. we have to pass the argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0888, -0.0024,  0.5353,  0.1906],\n",
      "        [-0.2281, -0.3698, -0.1026, -0.2641],\n",
      "        [-0.1962,  0.0293,  0.3651,  0.3328],\n",
      "        [-0.5986,  0.3796,  0.1711,  0.5809]], grad_fn=<SplitBackward0>)\n",
      "--------------------------------------------------------------------------------\n",
      "tensor([[ 0.4042, -0.5580, -0.5822, -0.2954],\n",
      "        [ 0.5377, -0.1020,  0.2621, -0.2846],\n",
      "        [ 0.6009, -0.2591,  0.4592,  0.0073],\n",
      "        [-0.3226,  0.3148, -0.3251,  0.1801]], grad_fn=<SplitBackward0>)\n",
      "--------------------------------------------------------------------------------\n",
      "tensor([[-0.1768, -0.0671, -0.5887, -0.2920],\n",
      "        [ 0.3323, -0.1489,  0.6100,  0.4909],\n",
      "        [-0.0287, -0.4087,  0.3729,  0.1901],\n",
      "        [-0.3958,  0.3978,  0.3718,  0.5431]], grad_fn=<SplitBackward0>)\n"
     ]
    }
   ],
   "source": [
    "W_q,W_k,W_v = multihead_attn.in_proj_weight.chunk(3)\n",
    "print(W_q)\n",
    "print('----'*20)\n",
    "print(W_k)\n",
    "print('----'*20)\n",
    "print(W_v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [False, False, False,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask = torch.triu(torch.ones(num_tokens,num_tokens),diagonal=1).bool()\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART ONE RESULT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch's nn.MultiheadAttention is designed to be flexible for different use cases, so it requires Q, K, and V as inputs rather than assuming they will be derived from the same source every time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART ONE RESULT :\n",
      "tensor([[[ 0.3536, -0.5042, -0.0514,  0.3310],\n",
      "         [ 0.2896, -0.5853, -0.0499,  0.3369],\n",
      "         [ 0.3767, -0.5305, -0.0452,  0.3406],\n",
      "         [ 0.1185, -0.5664, -0.0260,  0.2704]]])\n",
      "torch.Size([1, 4, 4])\n",
      "--------------------------------------------------------------------------------\n",
      "PART ONE ATTENTION OUTPUT WEIGHTS :\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5976, 0.4024, 0.0000, 0.0000],\n",
      "         [0.3300, 0.2603, 0.4097, 0.0000],\n",
      "         [0.2358, 0.2138, 0.2068, 0.3436]]])\n",
      "torch.Size([1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    PART_ONE_RESULT, attn_output_weights_part_one  = multihead_attn(input_to_produce_Q,\n",
    "                                                                    input_to_produce_K,\n",
    "                                                                    input_to_produce_V,\n",
    "                                                                    attn_mask = attn_mask )\n",
    "\n",
    "print('PART ONE RESULT :')\n",
    "print(PART_ONE_RESULT)\n",
    "print(PART_ONE_RESULT.shape)\n",
    "\n",
    "print('----'*20)\n",
    "\n",
    "print('PART ONE ATTENTION OUTPUT WEIGHTS :')\n",
    "print(attn_output_weights_part_one)    # Notice pytorch doesn't return attn_output_weights for each single head separately, rather it takes the average across the heads. \n",
    "print(attn_output_weights_part_one.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll use the weights which `nn.MultiheadAttention` initialized, in our `FromScratchMultiheadAttention`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART TWO : FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FromScratchMultiheadAttention(nn.Module):\n",
    "  def __init__(self,context_window,embed_dim,num_heads,dropout=0,add_bias_kv=False,device=None):\n",
    "    super().__init__()\n",
    "\n",
    "    # we will assume d_in == d_out and they are both embed_dim.\n",
    "\n",
    "    # Handling dimensions\n",
    "    assert embed_dim % num_heads == 0, 'Embedding must be divisible by Number of heads'\n",
    "    self.embed_dim = embed_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = self.embed_dim//self.num_heads\n",
    "\n",
    "    # W_q, W_k, W_v\n",
    "    self.W_q      = nn.Linear(embed_dim,embed_dim,bias=False)\n",
    "    self.W_k      = nn.Linear(embed_dim,embed_dim,bias=False)\n",
    "    self.W_v      = nn.Linear(embed_dim,embed_dim,bias=False)\n",
    "    self.out_proj = nn.Linear(embed_dim,embed_dim,bias=False)\n",
    "\n",
    "\n",
    "    W_q,W_k,W_v = multihead_attn.in_proj_weight.chunk(3)   # pytorch's internal initialization of nn.MultiheadAttention. pytorch initialize all three (q,k,v) in a single matrix.\n",
    "    out_proj = multihead_attn.out_proj.weight\n",
    "\n",
    "    # we gonna put the initialized weight by nn.MultiheadAttention to our layers. so we can see if they will produce the same result\n",
    "    self.W_q.weight.data = W_q\n",
    "    self.W_k.weight.data = W_k\n",
    "    self.W_v.weight.data = W_v\n",
    "    self.out_proj.weight.data = out_proj\n",
    "\n",
    "\n",
    "\n",
    "    # Miscellaneous\n",
    "    self.register_buffer('mask',torch.triu(torch.ones(context_window,context_window),diagonal=1))\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "  \n",
    "  def forward(self,input_to_produce_Q,input_to_produce_K,input_to_produce_V):\n",
    "    B_q,num_token_q,embed_dim_q = input_to_produce_Q.shape\n",
    "    B_k,num_token_k,embed_dim_k = input_to_produce_K.shape\n",
    "    B_v,num_token_v,embed_dim_v = input_to_produce_V.shape\n",
    "\n",
    "    Q = self.W_q(input_to_produce_Q) \n",
    "    K = self.W_k(input_to_produce_K) \n",
    "    V = self.W_v(input_to_produce_V) \n",
    "\n",
    "    # splitting (turn it to multi-head)\n",
    "    Q = Q.view(B_q,num_token_q,self.num_heads,self.head_dim).transpose(1,2)\n",
    "    K = K.view(B_k,num_token_k,self.num_heads,self.head_dim).transpose(1,2)\n",
    "    V = V.view(B_v,num_token_v,self.num_heads,self.head_dim).transpose(1,2)\n",
    "\n",
    "    # QK,mask,softmax,dropout\n",
    "    attn_score = Q @ K.transpose(2,3)\n",
    "    attn_score.masked_fill_(self.mask.bool()[:num_token_q,:num_token_k],-torch.inf)\n",
    "    attn_weight = torch.softmax(attn_score/K.shape[-1]**0.5,dim=-1)\n",
    "    attn_weight = self.dropout(attn_weight)\n",
    "\n",
    "    # context_vec\n",
    "    context_vec = attn_weight @ V\n",
    "\n",
    "    # Putting the heads back together \n",
    "    context_vec = context_vec.transpose(1,2).contiguous().view(B_q,num_token_q,self.embed_dim)    # it doesn't matter which (B) you choose\n",
    "\n",
    "    # projection \n",
    "    context_vec = self.out_proj(context_vec)\n",
    "\n",
    "    return context_vec,attn_weight\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART TWO RESULT  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can think of context_window as a max num_tokens your model can process at one go. since  we are feeding the model 8 tokens (numz-tokens = 8), context_window anything above 8 will do. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART TWO RESULT :\n",
      "tensor([[[ 0.3536, -0.5042, -0.0514,  0.3310],\n",
      "         [ 0.2896, -0.5853, -0.0499,  0.3369],\n",
      "         [ 0.3767, -0.5305, -0.0452,  0.3406],\n",
      "         [ 0.1185, -0.5664, -0.0260,  0.2704]]])\n",
      "torch.Size([1, 4, 4])\n",
      "--------------------------------------------------------------------------------\n",
      "PART TWO ATTENTION OUTPUT WEIGHTS :\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6564, 0.3436, 0.0000, 0.0000],\n",
      "          [0.3431, 0.2247, 0.4322, 0.0000],\n",
      "          [0.2558, 0.2385, 0.2509, 0.2548]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5388, 0.4612, 0.0000, 0.0000],\n",
      "          [0.3169, 0.2959, 0.3872, 0.0000],\n",
      "          [0.2158, 0.1890, 0.1628, 0.4324]]]])\n",
      "torch.Size([1, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "mha = FromScratchMultiheadAttention(context_window=1024,   # see the above note.\n",
    "                                    embed_dim=embed_dim,\n",
    "                                    num_heads=num_heads,\n",
    "                                    dropout=0.)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    PART_TWO_RESULT,attn_output_weights_part_two = mha(input_to_produce_Q,input_to_produce_K,input_to_produce_V)\n",
    "\n",
    "print('PART TWO RESULT :')\n",
    "print(PART_TWO_RESULT)\n",
    "print(PART_TWO_RESULT.shape)\n",
    "\n",
    "print('----'*20)\n",
    "\n",
    "print('PART TWO ATTENTION OUTPUT WEIGHTS :')\n",
    "print(attn_output_weights_part_two)    # Notice pytorch doesn't return attn_output_weights for each single head separately, rather it takes the average across the heads. \n",
    "print(attn_output_weights_part_two.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5976, 0.4024, 0.0000, 0.0000],\n",
       "         [0.3300, 0.2603, 0.4097, 0.0000],\n",
       "         [0.2358, 0.2138, 0.2068, 0.3436]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights_part_two = attn_output_weights_part_two.mean(dim=1)   # taking average across heads dimension.\n",
    "attn_output_weights_part_two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing `PART_ONE_RESULT` with `PART_TWO_RESULT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of PART ONE :\n",
      "\n",
      "tensor([[[ 0.3536, -0.5042, -0.0514,  0.3310],\n",
      "         [ 0.2896, -0.5853, -0.0499,  0.3369],\n",
      "         [ 0.3767, -0.5305, -0.0452,  0.3406],\n",
      "         [ 0.1185, -0.5664, -0.0260,  0.2704]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "Output of PART TWO :\n",
      "\n",
      "tensor([[[ 0.3536, -0.5042, -0.0514,  0.3310],\n",
      "         [ 0.2896, -0.5853, -0.0499,  0.3369],\n",
      "         [ 0.3767, -0.5305, -0.0452,  0.3406],\n",
      "         [ 0.1185, -0.5664, -0.0260,  0.2704]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "PART ONE is equal to PART TWO ? True\n"
     ]
    }
   ],
   "source": [
    "print(f'Output of PART ONE :\\n\\n{PART_ONE_RESULT}')\n",
    "print('---'*30)\n",
    "print(f'Output of PART TWO :\\n\\n{PART_TWO_RESULT}')\n",
    "print('---'*30)\n",
    "print(f'\\nPART ONE is equal to PART TWO ? {torch.allclose(PART_ONE_RESULT, PART_TWO_RESULT)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing `attn_output_weights_part_one` with `attn_output_weights_part_two`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of PART ONE :\n",
      "\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5976, 0.4024, 0.0000, 0.0000],\n",
      "         [0.3300, 0.2603, 0.4097, 0.0000],\n",
      "         [0.2358, 0.2138, 0.2068, 0.3436]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "Output of PART TWO :\n",
      "\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5976, 0.4024, 0.0000, 0.0000],\n",
      "         [0.3300, 0.2603, 0.4097, 0.0000],\n",
      "         [0.2358, 0.2138, 0.2068, 0.3436]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "PART ONE is equal to PART TWO ? True\n"
     ]
    }
   ],
   "source": [
    "print(f'Output of PART ONE :\\n\\n{attn_output_weights_part_one}')\n",
    "print('---'*30)\n",
    "print(f'Output of PART TWO :\\n\\n{attn_output_weights_part_two}')\n",
    "print('---'*30)\n",
    "print(f'\\nPART ONE is equal to PART TWO ? {torch.allclose(attn_output_weights_part_one, attn_output_weights_part_two)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afterwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feel comfortable proceeding with `nn.MultiheadAttention`, knowing exactly how the math is calculated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
