dropout: 0.1

# TOKENIZER
vocab_size: 50257   # tiktoken.get_encoding("gpt2").n_vocab
embed_dim: 768

# DATASET
stride: 256

# DATALOADER
batch_size: 2
shuffle: False
drop_last: True
num_workers: 0

# POSITIONAL ENCODING
# Durning training, num_tokens == context_window
# Durning inference, num_tokens <= context_window
context_window: 256

# Mulit-head attention
num_heads: 12
Q_K_V_bias: False
kv_bias: False
batch_first: True
device: null

# Transformer block
n_layers: 12

# num_classes 
num_classes: 50257 # for next token prediction must equal vocab_size, for classification must be the number of classes you have.

# generate
num_tokens_to_generate: 20
look_back: 256
text_to_generate: "Every single step"


# optimizer
learning_rate: 0.0004

# training
num_epochs: 10

