{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import yaml\n",
    "from pygments import highlight, lexers, formatters\n",
    "from typing import List,Dict,Tuple,Any\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import tiktoken\n",
    "\n",
    "from processing_data.dataset import InstructionDataset,format_input\n",
    "from processing_data.dataloader import get_data_loader,instruction_collate_fn\n",
    "from utils import tokens_to_text\n",
    "\n",
    "\n",
    "from gpt2 import GPT2Model\n",
    "from loss import cross_entropy\n",
    "from train import traininng_loop\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "with open('config.yaml','r') as f:\n",
    "    config = yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw_data/instruction-examples.json','r') as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"instruction\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"What is the plural form of \\\"goose\\\"?\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"input\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"output\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"The plural form of \\\"goose\\\" is \\\"geese.\\\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "}\u001b[37m\u001b[39;49;00m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_json = json.dumps(data[1], indent=4)\n",
    "colorful_json = highlight(formatted_json,\n",
    "                          lexers.JsonLexer(),\n",
    "                          formatters.TerminalFormatter())\n",
    "\n",
    "print(colorful_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the plural form of \"goose\"?\n"
     ]
    }
   ],
   "source": [
    "print(format_input(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 160\n",
      "Validation size: 20\n",
      "Test size: 20\n"
     ]
    }
   ],
   "source": [
    "train_index = int(len(data) * 0.8)\n",
    "val_index = int(len(data) * 0.1)\n",
    "\n",
    "train_data = data[:train_index]\n",
    "val_data = data[train_index: train_index + val_index]\n",
    "test_data = data[train_index + val_index:]\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = InstructionDataset(train_data,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the verb in the following sentence: The cat sleeps on the couch.\n",
      "\n",
      "### Responsive:\n",
      "The verb in the sentence is \"sleeps.\"\n"
     ]
    }
   ],
   "source": [
    "print(tokens_to_text(train_ds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = get_data_loader(\n",
    "    train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=instruction_collate_fn\n",
    "    )\n",
    "\n",
    "val_ds = InstructionDataset(val_data,tokenizer)\n",
    "val_dl = get_data_loader(\n",
    "    val_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=instruction_collate_fn\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.shape: torch.Size([2, 57]) output.shape: torch.Size([2, 57])\n",
      "input.shape: torch.Size([2, 57]) output.shape: torch.Size([2, 57])\n",
      "input.shape: torch.Size([2, 61]) output.shape: torch.Size([2, 61])\n",
      "input.shape: torch.Size([2, 60]) output.shape: torch.Size([2, 60])\n",
      "input.shape: torch.Size([2, 61]) output.shape: torch.Size([2, 61])\n",
      "input.shape: torch.Size([2, 57]) output.shape: torch.Size([2, 57])\n",
      "input.shape: torch.Size([2, 59]) output.shape: torch.Size([2, 59])\n",
      "input.shape: torch.Size([2, 63]) output.shape: torch.Size([2, 63])\n",
      "input.shape: torch.Size([2, 60]) output.shape: torch.Size([2, 60])\n",
      "input.shape: torch.Size([2, 64]) output.shape: torch.Size([2, 64])\n",
      "input.shape: torch.Size([2, 75]) output.shape: torch.Size([2, 75])\n",
      "input.shape: torch.Size([2, 60]) output.shape: torch.Size([2, 60])\n",
      "input.shape: torch.Size([2, 66]) output.shape: torch.Size([2, 66])\n",
      "input.shape: torch.Size([2, 58]) output.shape: torch.Size([2, 58])\n",
      "input.shape: torch.Size([2, 55]) output.shape: torch.Size([2, 55])\n",
      "input.shape: torch.Size([2, 59]) output.shape: torch.Size([2, 59])\n",
      "input.shape: torch.Size([2, 62]) output.shape: torch.Size([2, 62])\n",
      "input.shape: torch.Size([2, 61]) output.shape: torch.Size([2, 61])\n",
      "input.shape: torch.Size([2, 83]) output.shape: torch.Size([2, 83])\n",
      "input.shape: torch.Size([2, 68]) output.shape: torch.Size([2, 68])\n",
      "input.shape: torch.Size([2, 60]) output.shape: torch.Size([2, 60])\n",
      "input.shape: torch.Size([2, 65]) output.shape: torch.Size([2, 65])\n",
      "input.shape: torch.Size([2, 62]) output.shape: torch.Size([2, 62])\n",
      "input.shape: torch.Size([2, 67]) output.shape: torch.Size([2, 67])\n",
      "input.shape: torch.Size([2, 61]) output.shape: torch.Size([2, 61])\n",
      "input.shape: torch.Size([2, 63]) output.shape: torch.Size([2, 63])\n",
      "input.shape: torch.Size([2, 60]) output.shape: torch.Size([2, 60])\n",
      "input.shape: torch.Size([2, 63]) output.shape: torch.Size([2, 63])\n",
      "input.shape: torch.Size([2, 64]) output.shape: torch.Size([2, 64])\n",
      "input.shape: torch.Size([2, 60]) output.shape: torch.Size([2, 60])\n",
      "input.shape: torch.Size([2, 69]) output.shape: torch.Size([2, 69])\n",
      "input.shape: torch.Size([2, 60]) output.shape: torch.Size([2, 60])\n",
      "input.shape: torch.Size([2, 68]) output.shape: torch.Size([2, 68])\n",
      "input.shape: torch.Size([2, 62]) output.shape: torch.Size([2, 62])\n",
      "input.shape: torch.Size([2, 65]) output.shape: torch.Size([2, 65])\n",
      "input.shape: torch.Size([2, 64]) output.shape: torch.Size([2, 64])\n",
      "input.shape: torch.Size([2, 51]) output.shape: torch.Size([2, 51])\n",
      "input.shape: torch.Size([2, 64]) output.shape: torch.Size([2, 64])\n",
      "input.shape: torch.Size([2, 70]) output.shape: torch.Size([2, 70])\n",
      "input.shape: torch.Size([2, 72]) output.shape: torch.Size([2, 72])\n",
      "input.shape: torch.Size([2, 62]) output.shape: torch.Size([2, 62])\n",
      "input.shape: torch.Size([2, 92]) output.shape: torch.Size([2, 92])\n",
      "input.shape: torch.Size([2, 67]) output.shape: torch.Size([2, 67])\n",
      "input.shape: torch.Size([2, 74]) output.shape: torch.Size([2, 74])\n",
      "input.shape: torch.Size([2, 64]) output.shape: torch.Size([2, 64])\n",
      "input.shape: torch.Size([2, 63]) output.shape: torch.Size([2, 63])\n",
      "input.shape: torch.Size([2, 71]) output.shape: torch.Size([2, 71])\n",
      "input.shape: torch.Size([2, 78]) output.shape: torch.Size([2, 78])\n",
      "input.shape: torch.Size([2, 58]) output.shape: torch.Size([2, 58])\n",
      "input.shape: torch.Size([2, 79]) output.shape: torch.Size([2, 79])\n",
      "input.shape: torch.Size([2, 84]) output.shape: torch.Size([2, 84])\n",
      "input.shape: torch.Size([2, 63]) output.shape: torch.Size([2, 63])\n",
      "input.shape: torch.Size([2, 62]) output.shape: torch.Size([2, 62])\n",
      "input.shape: torch.Size([2, 60]) output.shape: torch.Size([2, 60])\n",
      "input.shape: torch.Size([2, 60]) output.shape: torch.Size([2, 60])\n",
      "input.shape: torch.Size([2, 70]) output.shape: torch.Size([2, 70])\n",
      "input.shape: torch.Size([2, 62]) output.shape: torch.Size([2, 62])\n",
      "input.shape: torch.Size([2, 61]) output.shape: torch.Size([2, 61])\n",
      "input.shape: torch.Size([2, 69]) output.shape: torch.Size([2, 69])\n",
      "input.shape: torch.Size([2, 66]) output.shape: torch.Size([2, 66])\n",
      "input.shape: torch.Size([2, 73]) output.shape: torch.Size([2, 73])\n",
      "input.shape: torch.Size([2, 71]) output.shape: torch.Size([2, 71])\n",
      "input.shape: torch.Size([2, 57]) output.shape: torch.Size([2, 57])\n",
      "input.shape: torch.Size([2, 65]) output.shape: torch.Size([2, 65])\n",
      "input.shape: torch.Size([2, 70]) output.shape: torch.Size([2, 70])\n",
      "input.shape: torch.Size([2, 64]) output.shape: torch.Size([2, 64])\n",
      "input.shape: torch.Size([2, 75]) output.shape: torch.Size([2, 75])\n",
      "input.shape: torch.Size([2, 74]) output.shape: torch.Size([2, 74])\n",
      "input.shape: torch.Size([2, 65]) output.shape: torch.Size([2, 65])\n",
      "input.shape: torch.Size([2, 56]) output.shape: torch.Size([2, 56])\n",
      "input.shape: torch.Size([2, 59]) output.shape: torch.Size([2, 59])\n",
      "input.shape: torch.Size([2, 59]) output.shape: torch.Size([2, 59])\n",
      "input.shape: torch.Size([2, 62]) output.shape: torch.Size([2, 62])\n",
      "input.shape: torch.Size([2, 65]) output.shape: torch.Size([2, 65])\n",
      "input.shape: torch.Size([2, 66]) output.shape: torch.Size([2, 66])\n",
      "input.shape: torch.Size([2, 61]) output.shape: torch.Size([2, 61])\n",
      "input.shape: torch.Size([2, 60]) output.shape: torch.Size([2, 60])\n",
      "input.shape: torch.Size([2, 72]) output.shape: torch.Size([2, 72])\n",
      "input.shape: torch.Size([2, 69]) output.shape: torch.Size([2, 69])\n",
      "input.shape: torch.Size([2, 63]) output.shape: torch.Size([2, 63])\n"
     ]
    }
   ],
   "source": [
    "for input,output in train_dl:\n",
    "    print(f'input.shape: {input.shape} output.shape: {output.shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dl))\n",
    "print(len(val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input,output in train_dl:\n",
    "#     print(input)\n",
    "#     print(output)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4223,  0.8664, -0.9820,  ..., -1.6785, -0.8263, -0.5373],\n",
      "         [-0.9225,  0.3052, -0.6632,  ..., -1.4212,  0.5825,  1.2669],\n",
      "         [ 0.1208,  0.0786, -1.1472,  ..., -0.0067, -0.7273,  1.0709],\n",
      "         ...,\n",
      "         [-0.5586, -0.2207, -0.0293,  ..., -0.7779, -0.1665, -0.2204],\n",
      "         [-0.5827, -1.2379, -0.4539,  ..., -0.5088,  1.3019, -0.2421],\n",
      "         [ 0.4074, -0.0570, -0.0421,  ...,  0.9512,  0.6207,  0.3603]],\n",
      "\n",
      "        [[-0.1490,  0.5916, -0.9294,  ..., -0.5167, -0.7071, -0.1347],\n",
      "         [-0.2227,  0.3213,  0.1298,  ..., -0.6169,  0.2928, -0.0327],\n",
      "         [-0.5166,  0.6594, -0.5260,  ...,  0.5458, -0.3208,  1.5027],\n",
      "         ...,\n",
      "         [-1.0470, -0.2002,  0.0915,  ...,  0.0203, -0.2529, -0.3517],\n",
      "         [-0.1029, -0.6388, -0.4151,  ..., -0.0661,  0.6080,  0.4242],\n",
      "         [ 1.5079,  0.1866,  1.0460,  ..., -0.3795,  0.1673,  0.2957]]])\n"
     ]
    }
   ],
   "source": [
    "model = GPT2Model(config)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(torch.randint(0,100,(2,10)))\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout': 0.1,\n",
       " 'vocab_size': 50257,\n",
       " 'embed_dim': 768,\n",
       " 'stride': 256,\n",
       " 'batch_size': 2,\n",
       " 'shuffle': False,\n",
       " 'drop_last': True,\n",
       " 'num_workers': 0,\n",
       " 'context_window': 256,\n",
       " 'num_heads': 12,\n",
       " 'Q_K_V_bias': False,\n",
       " 'kv_bias': False,\n",
       " 'batch_first': True,\n",
       " 'device': None,\n",
       " 'n_layers': 12,\n",
       " 'num_classes': 50257,\n",
       " 'num_tokens_to_generate': 20,\n",
       " 'look_back': 256,\n",
       " 'text_to_generate': 'Every single step',\n",
       " 'learning_rate': 0.0004,\n",
       " 'num_epochs': 10}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 20:45:37,652 - INFO - Epoch 1/10\n",
      "2025-05-05 20:46:26,078 - INFO - Seen tokens: 10374\n",
      "2025-05-05 20:46:26,079 - INFO - Loss: 2.3498\n",
      "2025-05-05 20:46:26,692 - INFO - Validation Loss: 3.3537\n",
      "2025-05-05 20:46:26,693 - INFO - ==================================================\n",
      "2025-05-05 20:46:26,693 - INFO - Epoch 2/10\n",
      "2025-05-05 20:47:10,955 - INFO - Seen tokens: 20748\n",
      "2025-05-05 20:47:10,956 - INFO - Loss: 2.1597\n",
      "2025-05-05 20:47:11,577 - INFO - Validation Loss: 3.3766\n",
      "2025-05-05 20:47:11,577 - INFO - ==================================================\n",
      "2025-05-05 20:47:11,578 - INFO - Epoch 3/10\n",
      "2025-05-05 20:47:48,259 - INFO - Seen tokens: 31122\n",
      "2025-05-05 20:47:48,261 - INFO - Loss: 2.0995\n",
      "2025-05-05 20:47:48,909 - INFO - Validation Loss: 3.3921\n",
      "2025-05-05 20:47:48,909 - INFO - ==================================================\n",
      "2025-05-05 20:47:48,909 - INFO - Epoch 4/10\n",
      "2025-05-05 20:48:36,046 - INFO - Seen tokens: 41496\n",
      "2025-05-05 20:48:36,048 - INFO - Loss: 2.0630\n",
      "2025-05-05 20:48:36,670 - INFO - Validation Loss: 3.4224\n",
      "2025-05-05 20:48:36,671 - INFO - ==================================================\n",
      "2025-05-05 20:48:36,671 - INFO - Epoch 5/10\n",
      "2025-05-05 20:49:23,169 - INFO - Seen tokens: 51870\n",
      "2025-05-05 20:49:23,171 - INFO - Loss: 2.0232\n",
      "2025-05-05 20:49:23,808 - INFO - Validation Loss: 3.4492\n",
      "2025-05-05 20:49:23,808 - INFO - ==================================================\n",
      "2025-05-05 20:49:23,809 - INFO - Epoch 6/10\n",
      "2025-05-05 20:50:14,377 - INFO - Seen tokens: 62244\n",
      "2025-05-05 20:50:14,380 - INFO - Loss: 1.9898\n",
      "2025-05-05 20:50:15,006 - INFO - Validation Loss: 3.4791\n",
      "2025-05-05 20:50:15,006 - INFO - ==================================================\n",
      "2025-05-05 20:50:15,007 - INFO - Epoch 7/10\n",
      "2025-05-05 20:51:00,298 - INFO - Seen tokens: 72618\n",
      "2025-05-05 20:51:00,300 - INFO - Loss: 1.9654\n",
      "2025-05-05 20:51:00,923 - INFO - Validation Loss: 3.4994\n",
      "2025-05-05 20:51:00,924 - INFO - ==================================================\n",
      "2025-05-05 20:51:00,925 - INFO - Epoch 8/10\n",
      "2025-05-05 20:51:39,263 - INFO - Seen tokens: 82992\n",
      "2025-05-05 20:51:39,265 - INFO - Loss: 1.9361\n",
      "2025-05-05 20:51:39,891 - INFO - Validation Loss: 3.5274\n",
      "2025-05-05 20:51:39,892 - INFO - ==================================================\n",
      "2025-05-05 20:51:39,892 - INFO - Epoch 9/10\n",
      "2025-05-05 20:52:24,419 - INFO - Seen tokens: 93366\n",
      "2025-05-05 20:52:24,421 - INFO - Loss: 1.9184\n",
      "2025-05-05 20:52:25,054 - INFO - Validation Loss: 3.5514\n",
      "2025-05-05 20:52:25,055 - INFO - ==================================================\n",
      "2025-05-05 20:52:25,055 - INFO - Epoch 10/10\n",
      "2025-05-05 20:53:08,012 - INFO - Seen tokens: 103740\n",
      "2025-05-05 20:53:08,014 - INFO - Loss: 1.9098\n",
      "2025-05-05 20:53:08,651 - INFO - Validation Loss: 3.5875\n",
      "2025-05-05 20:53:08,652 - INFO - ==================================================\n"
     ]
    }
   ],
   "source": [
    "traininng_loop(\n",
    "    model,\n",
    "    train_dl,\n",
    "    val_dl,\n",
    "    cross_entropy,\n",
    "    optimizer,\n",
    "    num_epochs=10,\n",
    "    device='cpu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
