dropout: 0.1

# TOKENIZER
vocab_size: 50257   # tiktoken.get_encoding("gpt2").n_vocab
embed_dim: 768

# DATASET
stride: 256

# DATALOADER
batch_size: 96
shuffle: False
drop_last: True
num_workers: 4

# POSITIONAL ENCODING
# Durning training, num_tokens == context_window
# Durning inference, num_tokens <= context_window
context_window: 502

# Mulit-head attention
num_heads: 12
Q_K_V_bias: False
kv_bias: False
batch_first: True
device: "cuda"

# Transformer block
n_layers: 12

# num_classes 
num_classes: 50304    #Originally  50257 (Since it is an "ugly Number" we change it with next(i.e. Higher) pretty Number)

# optimizer
learning_rate: 0.0003

# training
epochs: 1
log_ever_n_batches: 50

